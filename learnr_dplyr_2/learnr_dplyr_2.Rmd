---
title: "Data Cleaning with dplyr (2)"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(rsdmx)
knitr::opts_chunk$set(echo = FALSE)

# Loading conflict data from ICDP
temp <- tempfile() # opens empty temporary file
download.file("https://correlatesofwar.org/data-sets/national-material-capabilities/nmc-v5-1/at_download/file", temp) # downloads and saves into 'temp'
cow <- read.csv(unz(temp, # unzips & reads CSV file
                    "NMC_5_0.csv"),
                header = T)
unlink(temp)
rm(temp)

```

## Introduction

In the previous tutorial, you learned how to do two important data cleaning operations with the `dplyr`-package: keeping (or dropping) variables using the `select()`-function, and keep or dropping observations based on some criteria using `filter()`. You also learned how you can efficiently execute several such commands in sequence by using the pipe-operator `%>%`.

With these commands, you can do some basic data cleaning and trim your dataset to what you really need for your analysis. But, obviously, this gets you only so far and most data cleaning and management also involves creating new variables, recoding existing ones, aggregating the data (e.g. into average values) as a whole or also for some groups of observations.

You will learn how to do these operations in this tutorial. And, as in the previous tutorial, we will rely mostly on functions from the `dplyr` package as well as the pipe operator `%>%`.

Before we get started, let's quickly repeat what the pipe operator does and how you use it.

## A quick recap of the `%>%`

This tutorial will make heavy use of the pipe operator `%>%`, so it is important to quickly repeat what it does. In essence, the pipe operator links together multiple functions in a row --- it tells R to take the result from one operation and feed it directly into the next one, and so on. By linking together multiple operations in this way, we can build "data management pipelines", which take a messy dataset at one end, clean it, and then spit out a clean dataset at the other end.

To make matters more concrete, here is a piece of code that shows the pipe in action:
```{r pipeex, exercise=T}
cpds %>% # 1.
  select(country,year,unemp) %>%  # 2.
  filter(year>=1990) %>%  # 3.
  head() # 4.
```

In human language:

1. "Take the `cpds` object (the *Comparative Political Dataset*, in this case),..."
2. "...select three variables, `country`, `year`, and `unemp` (the unemployment rate variable) from it and drop the rest,..."
3. "...then keep only those observations that relate to 1990 and after,..."
4. "...and finally print the first few observations."

Notice that, because we use `%>%` operator, we can use the `head()` function by itself, without further details. R knows that we want to take the result from the previous steps here.

Got it? Great, then let's learn how to create new variables!

## Creating new variables with `mutate()`




## Summarizing with `summarize()`

## Grouping with `group()`

## Recoding with `recode()`